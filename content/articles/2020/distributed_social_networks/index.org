#+title: Distributed Social Networks
#+subtitle: scuttlebutt is awesome
#+tags: p2p, design, overview
#+date: 2020-06-26
#+startup:inlineimages
#+draft: true

Lets think about how we'd build an in-browser distributed social
network.  Basically, let's rebuild scuttlebutt so that it uses browser
APIs only.

* Content addressable

Every bit of content is named by its signature, so therefor the data
doesn't live anywhere.  The private key is the only special
information, and perhaps this could be a mnenomic like "memorize your
bitcoin wallet address with this weird sentence thing".

Machines connect, cache and share the data that they have. Like NNTP,
it starts out with a IWANT and IHAVE negotiation to get the heads, and
then pulls stuff down as needed.

Larger blobs are stored the same way, so there's an underlying object
cloud there.  Blob size is shared as part of the protocol, so when
available blobs are announced you know how big it is.

* Identity
Identity is generated by a key on startup.  There is a private key
that signs everything and you "are" your public key.

Entities are basically a key and a published head entry.  This entry
may or may not require a read key (i.e. for private feeds) and it may
contain encrytped entries.  Each entry points to a previous entry.

* Synods
Identities can be grouped into synods, which is a collection of
mutually signed identities.  Multiple devices with mutliple identities
with the same owner can be considered a synods.

Synods are also used to retire identities, for example if a device was
lost or the identity was compremised.  In this case the identity would
mark the last valid head of a particular identity and no other heads
than that are considered valid and therefor not propogated.

Synods are basically another type of feed that contains pointers to
shared identities, a list of used signal servers, and profile
information.

* Chain

Every entry is a json blob which points to its previous blob. The key
of the blog is its hashkey, so that you can ask for an id and verify
that the data is correct. It doesn't matter who you get the data from.

All entries are signed by the public key of the identity, so you know
that it's from the same private key.

** Valid feeds

Invalid feeds don't propogate

1. Feed that are too large for the network to propogate. (e.g. you can
   post a link to a movie but not the movie itself)
2. Feeds that contain invalidly signed entries.
3. Feeds that contain images with exif data
4. Feeds that have a revoked public key
5. What else

Question: Is there a way to revoke keys?

** Heads

Each node keeps track of identities and heads, which is the latest
entry. When a node connects, it announces all of the identities that
are tracked, and what the latest head it has.  Nodes can then share
and request heads, and if they have the feed read keys they can trace
things backwards to get to the root.

* Entries

Data, signature, identity, localtime and synod.  Data is either an
encrypted blob of json, or it is regular json.

If you get a entities for an identity that you don't know, you can use
the public key to request the feed head. Does it make sense to cache
the profile with the feed head, something like "latest
identification"?  From here you can pull down the head and follow
things backwards to connect to the feed of the person.

Is there a common getBlob interface that does all of this?

** Types

| identification | json with name, about, whatever                                        |
| public_note    | note to the world                                                      |
| private_note   | note to people with readkeys                                           |
| image          | pointer to image blob                                                  |
| collection     | pointer to archive of blobs                                            |
| tag            | marking an entry in some way                                           |
| comment        | Make a comment to on an entry                                          |
| follow         | you announce that you are following a feed                             |
| block          | you tell the system you no longer are tracking and sharing an identity |
| mute           | encrypted block that pretends not to know anything                     |

** References
Referencing other entries are to its hash, which contains a pointer to
the identity and the synod. Both the identity and the synod can be
referenced with the blob request mechanism.  Once these are loaded,
the system then uses the identity,head from the signalling server or
connected clients to reconstruct the feed, whose entries are validated
using the signatures.

References are not to the feed itself, but to an entry on the feed.
So its possible to find references to an entity, locate the last
version of it's feed but not find the original entity on it.

* Multiencryption

How does SSB have messages that multiple people can decrypt?  This is
better than having any meta data since only the people that it's for
are able to see anything about it.

* Feed

Everyone has a head, which is the latest that the feed has.  Each
entry is a type and a pointer to a previous feed.

TODO A feed contains an ordered list to a number of entries, and also a
pointer to a previous feed list.  This is to reduce the number of
requests beween servers.  List is split based upon keeping blob size
small.

Deletion is done with rebasing, which means that you are able to
rewrite your feed.  So content that you address for commenting or
whatever needs to be seperate from the feed itself, and in theory you
could comment upon a feed entry that is missing from the head. These
orphaned contents will always point to an identity, and the datablobs
are the same, but may not be discoverable.

* Bulletin Board

The identity should announce endpoints where it receives messages. All
messages should be encrypted and only viewable with the private
key. The system should pull down messages from the inboxes -- which
can be public places that automatically delete everything after 14
days -- and then you can choose to pull something down or not.

Note that this is basically an SSB pub that will automatically echo
things to a specific identity.  The question is then, does the
identity choose to follow that new identity, or to announce it?  If
yes it enters the system.  Otherwise nothing happens.

These messages are public bulletin boards.  These signally servers
have feeds that they post of messages that people can get. TODO

* Sequence
** Startup

When a client starts up, it connects to all of the signaling servers
in it's database and announces that it's online, it's identity, and
the head of the feed.  The signaling server then responds with it's
identity and head, and also notifies all of the other clients that it
sees a new identity on the network with the identity and head.

The client can also request the last known heads for a group of
identities. Signal servers maintain a list for the previous 30 days.
Unless a data lease was requested from the server, the blobs won't be
available.

#+begin_src plantuml :file startup.png
scale 800 width

client -> signalserver : online,identity,head
signalserver -> client : online,identity,head
signalserver -> client : presence_list,[identity,head]
signalserver -> onlineclients : online,identity,head
client -> signalserver : getHeads(identities)
signalserver -> client : lastKnownHead(identities)
#+end_src

#+RESULTS:
[[file:startup.png]]

** Signal Server

A signal server forwards online messages to things that are connected
to it, using websockets.  The server periodically pings each client to
see if it's alive, and if that fails or there's a network error it
sends a broken connection to the client.

#+begin_src plantuml :file signaling.png
scale 800 width

signalserver -> client : online,identity,head
signalserver -> client : ping(timestamp)
client -> signalserver : pong(timestamp)
signalserver -> onlineclients : disconnect,identity
#+end_src

#+RESULTS:
[[file:signaling.png]]

** Connecting

A client maintains a list of identities that it's tracking. When it
gets a message from a signal server that the client is online, it
tries to connect over WebRTC to the client.

In this case all that the signal server is tracking is the connection
info and identity of the device.

A session token is created a signed by both parties to track who is
requesting what from whom.  

#+begin_src plantuml :file p2pconnect.png
scale 800 width
client1 -> signalserver : online,identity,head
signalserver -> client1 : online,identity,head
client2 -> signalserver : online,identity,head
signalserver -> client2 : online,identity,head
client1 -> signalserver : offer,client2.identity
signalserver -> client2 : offer
client2 -> signalserver : answer
signalserver -> client1 : answer
client1 -> client2 : online,identity,head
client2 -> client1 : online,identity,head
client1 -> client2 : getHeads(identities)
client2 -> client1 : lastKnownHead(identities)
client2 -> client1 : getHeads(identities)
client1 -> client2 : lastKnownHead(identities)
#+end_src

#+RESULTS:
[[file:p2pconnect.png]]
** I Want List
Once the client is connected to a system, signal or client, it sends a
list of the blobs that it wants.  Since these are content addressable
and signed by the identity, it doesn't matter where they come from.

A signal server may or may not have blobs -- its a regular client that
presumably is free of filewall mess, and had the additional feature of
being able to relay requests.

The client looks through its list of identities, and all of the head
announcements that it's received.  For each of these it tries to get
the blob associate with the head.

#+begin_src plantuml :file flowchart.png
scale 800 width

(*) --> "Receive identity,head message"

If "tracking identity" then
  -->[Yes] "get blob"
else
  -->[No] "ignore"
Endif

"get blob" If "has blob" then
  -->[Yes] If "decrypt blob" then
    -->[get parent] "get blob"
  else
    -->[No] "ignore"
  Endif

else
  ->[No] "add to iwantlist"
Endif

--> (*)

#+end_src

#+RESULTS:
[[file:flowchart.png]]

#+begin_src plantuml :file hasblob.png
scale 800 width

client -> localrepository : getblob
localrepository -> client : noblob
client -> connections : getblob(maxsize)
connections -> client : blob
client -> localrepository : storeblob
#+end_src

#+RESULTS:
[[file:hasblob.png]]
** Blob propogation

Network latecy and bandwidth is measured on all client connections.
Clients periodically send iwant lists to each other, returning the
blobs that it has or is willing to share (based on perhaps bandwidth,
if the client is operating on a battery, or over a metered celluar
connection.)

Clients should track incoming blob requests and outgoing blog
bandwidth to create a leach ratio that takes into account sharing
reciprocity. Credit is created by sharing more or perhaps by
purchasing bandwidth from the remote server. The request is signed by
the identity so the clients can tell who is asking for what.

#+begin_src plantuml :file blobpropgation.png
scale 800 width

clienta->clientb: iwant(blobs)
clientb->clienta: ihave(blobs)
clienta->clientb: getblob(blob,maxsize)
clientb->clienta: blob

#+end_src

#+RESULTS:
[[file:blobpropgation.png]]
** Services

Each client is able to provide services for other ones outside of blob
propogation, which all clients are required to provide. These requests
require a session token (signed by the identity) to prevent replaying.

| Service    | Description                                            |
| signalling | Network presence                                       |
| relaying   | passing data to a mutually connection not directly     |
| dropbox    | Receives and forwards requests from unknown identities |
| data lease | Storing of blobs with some guarentees                  |
| voice      | Voice calling                                          |
| video      | Video calling                                          |

Data leasing is an interesting example, which could be used to backup
information on a cluster of mutually owned devices, or to be able to
backup your data on your friends phones.

#+begin_src plantuml :file services.png
scale 800 width
clienta->clientb: wantservice(sessiontoken)
clientb->clienta: providesservice(sessiontoken)
clienta->clientb: call(service,datalease,head)
#+end_src

#+RESULTS:
[[file:services.png]]
** Chain validity
1. All entries need to be less than 15K. TODO
2. All unencrypted entries to photos must not have location data.
3. All head requests with an synod identity with a final head must be
   marked invalid.
