---
title: Why bother with automated tests
subtitle: What does it mean when a test fails?
# date: TBD When publishing
tags: testing, overview, tools
---

What is the reason write automated software tests?  When you check out some code for the first time and see that everything in the `test` directory is autogenerated, unimplemented `pendings`, does it follow that everything else in there is going to be junk?  Is percentage test coverage a meaningful metric?

> "Debugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it." --Brian Kernighan


Postulate 1: Tests are an investment that pays off technical debt
Postulate 2: Writing tests leads to better code
Postulate 3: When you refactor code, tests give you confidence in your changes
Postulate 4: Having a comprehensive test suite gives you confidence in your deploys

Writing tests are useful to help think through the design. Running tests are useful to check for regressions and if something you are looking to break does break. Proponents of test driven development describe it as a absolute necessity which will create software obviously better without any sort of trade-offs. This is a questionable position.


If you compare the amount of code in a decent test suite to the code you are testing generally there's significantly more test code. All code has bugs, and if there's more test code under his application code, then logically more bugs will exist inside of your test code then they will inside of your application code.

On top of that, writing tests is actually very difficult. Setting up the test environment itself, to be able to simulate distinct parts of your application requires you to understand much more of the implementation details of the underlying system then you need to know to accurately use that system. For example, testing devise authentication controllers requires a deep understanding of the of how rails, rack and warden work in order to simulate a test request, way more then you need to know to simply write correct devise production code. You not only need to know how to use your tools, but you need to really know how to use your tools to test those tools. Mocks and doubles bring in a whole other level of abstract tools to blow your mind with.

Writing tests is difficult in a different sense: what does it mean to write a test that fails or passes? It's sort of like trying to design a science experiment, where you need to construct a world where getting one result or another will approve or disapprove a hypothesis. And given that a large part of running a test is to construct the minimum amount of data that needs to exist in order to have the context in which the test is run be representative of reality, that's a lot of implicit knowledge that is embodied at that part of the stage. In the context of the quote above, this means that the test code is more clever than the code it's testing, and clever code is bad code.

A common type of test bug that I see, as an example, is cleverness inside of how factory fixtures are used to create a complicated data objects graph that need to exist for some functionality to be tested. Default values inside of the factory don't properly mimic how this data will be created in production, since inside the fixtures there's really no way to distinguish between fields that are functionally irrelevant content fields versus fields which have logic based off of them. So while the tests pass or fail based upon the code that is being tested correct or not correct given some data, the tests themselves don't really represent what they claim to because there are confounding factors, that the data isn't reflective of the range of production data. In other words, it's a poorly designed experiment.

Testing is then both practically, and theoretically, difficult. Additionally it's a lot of work and lumbering weight that you need to maintain and continually re-factor as new features get developed. The costs of writing and maintaining test is clearly not zero.

So then, what does a passing test suite really tell you? What does a failing test suite tell you? Is it really worth it?

I'm not sure that in general you can say anything about them. It's sort of like the argument that a static type system will prevent a class of bugs, that writing code in Haskell is inherently higher quality than PHP. Which is ridiculous, there's no world where the human is taken out of the quality equation.

The value in writing tests are in the writing itself. 
When you write tests, especially if you follow the TDD process, then it makes it very easy to structure your thinking and enter into an incredibly addictive flow state at will. Where you can enter in a fully focused zone working and continually improving the code in a structured way. 

The actual value is a side effect. The code is improved because you were forced to write code which is testable. Testable code has a overlap between good code; code which is testable is code that is fully encapsulated, isolated, has and behaves to a explicit contracts and is well structured.

It also gives you a great space to think through different scenarios of the code. There are situations that you can easily constructs with the tests to see what your code does that will be very difficult for someone approaching in as a black box, to analyze.

Having the tests are of limited, or at least a greatly reduced, value. I tend to focus my testing on models, controllers, and feature test. And after I write the model tests, and to some extent the controller tests, they never really fail after the process of getting them to pass and then refactoring happens. I don't think that a passing test suite of models or controllers actually gives you any information at all. Failure is your information, but when they pass I don't think you know anything more than you did before you run the tests. I'm not totally convinced that you can expect to refactor major parts of the code without factoring the tests. There's some refactors that do in fact change the contract between the app and test code.

Integration tests, however, are vitally valuable for regression testing. Those tests on the pass to tell you something. But of course it depends upon how well the test themselves are designed. Because they had more parts of the system, and are therefore more likely to change the pass or fail status based upon other parts of the system, a passing test contains more information.

